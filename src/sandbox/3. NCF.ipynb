{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import constants\n",
    "from utils import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(constants.RATINGS_PATH_SANDBOX, parse_dates=[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In train propouses we will use only 30% of all ratings dataset\n",
    "# rand_userIds = np.random.choice(\n",
    "#     ratings[\"userId\"].unique(),\n",
    "#     size=int(len(ratings[\"userId\"].unique()) * 0.3),\n",
    "#     replace=False,\n",
    "# )\n",
    "\n",
    "# ratings = ratings.loc[ratings[\"userId\"].isin(rand_userIds)]\n",
    "# print(\"There are {} rows of data from {} users\".format(len(ratings), len(rand_userIds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings, test_ratings = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the dataset into an implicit feedback dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings.loc[:, \"rating\"] = 1\n",
    "test_ratings.loc[:, \"rating\"] = 1\n",
    "\n",
    "\n",
    "train_ratings.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates 4 negative samples for each row of data. In other words, the ratio of negative to positive samples is 4:1. This ratio is chosen arbitrarily but I found that it works rather well (feel free to find the best ratio yourself!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all movie IDs\n",
    "all_movieIds = ratings[\"movieId\"].unique()\n",
    "\n",
    "users, items, labels = [], [], []\n",
    "\n",
    "# This is the set of items that each user has interaction with\n",
    "user_item_set = set(zip(train_ratings[\"userId\"], train_ratings[\"movieId\"]))\n",
    "\n",
    "# 4:1 ratio of negative to positive samples\n",
    "num_negatives = 4\n",
    "\n",
    "for u, i in tqdm(user_item_set):\n",
    "    users.append(u)\n",
    "    items.append(i)\n",
    "    labels.append(1)  # items that the user has interacted with are positive\n",
    "    for _ in range(num_negatives):\n",
    "        # randomly select an item\n",
    "        negative_item = np.random.choice(all_movieIds)\n",
    "        # check that the user has not interacted with this item\n",
    "        while (u, negative_item) in user_item_set:\n",
    "            negative_item = np.random.choice(all_movieIds)\n",
    "        users.append(u)\n",
    "        items.append(negative_item)\n",
    "        labels.append(0)  # items not interacted with are negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My custom dataset (MovieLense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensTrainDataset(Dataset):\n",
    "    \"\"\"MovieLens PyTorch Dataset for Training\n",
    "\n",
    "    Args:\n",
    "        ratings (pd.DataFrame): Dataframe containing the movie ratings\n",
    "        all_movieIds (list): List containing all movieIds\n",
    "        is_training (bool): Default is True. Indicate for progress bar\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratings, all_movieIds, is_training: bool = True):\n",
    "        self.is_training = is_training\n",
    "        self.users, self.items, self.labels = self.get_dataset(ratings, all_movieIds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "    def get_dataset(self, ratings, all_movieIds):\n",
    "        users, items, labels = [], [], []\n",
    "        user_item_set = set(zip(ratings[\"userId\"], ratings[\"movieId\"]))\n",
    "\n",
    "        num_negatives = 4\n",
    "        for u, i in tqdm(\n",
    "            user_item_set,\n",
    "            desc=f\"Generating negative sample for {'training' if self.is_training else 'validating'}\",\n",
    "        ):\n",
    "            users.append(u)\n",
    "            items.append(i)\n",
    "            labels.append(1)\n",
    "            for _ in range(num_negatives):\n",
    "                negative_item = np.random.choice(all_movieIds)\n",
    "                while (u, negative_item) in user_item_set:\n",
    "                    negative_item = np.random.choice(all_movieIds)\n",
    "                users.append(u)\n",
    "                items.append(negative_item)\n",
    "                labels.append(0)\n",
    "\n",
    "        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(pl.LightningModule):\n",
    "    \"\"\"Neural Collaborative Filtering (NCF)\n",
    "\n",
    "    Args:\n",
    "        num_users (int): Number of unique users\n",
    "        num_items (int): Number of unique items\n",
    "        ratings (pd.DataFrame): Dataframe containing the movie ratings for training\n",
    "        all_movieIds (list): List containing all movieIds (train + test)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_users, num_items, ratings, all_movieIds):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ratings = ratings\n",
    "        self.all_movieIds = all_movieIds\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=8)\n",
    "        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=8)\n",
    "        self.fc1 = nn.Linear(in_features=16, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.output = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        # Pass through embedding layers\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        item_embedded = self.item_embedding(item_input)\n",
    "\n",
    "        # Concat the two embedding layers\n",
    "        vector = torch.cat([user_embedded, item_embedded], dim=-1)\n",
    "\n",
    "        # Pass through dense layer\n",
    "        vector = nn.ReLU()(self.fc1(vector))\n",
    "        vector = nn.ReLU()(self.fc2(vector))\n",
    "\n",
    "        # Output layer\n",
    "        pred = nn.Sigmoid()(self.output(vector))\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_input, item_input, labels = batch\n",
    "        predicted_labels = self(user_input, item_input)\n",
    "        loss = nn.BCELoss()(predicted_labels, labels.view(-1, 1).float())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            MovieLensTrainDataset(self.ratings, self.all_movieIds, is_training=True),\n",
    "            batch_size=constants.NCF_BATCH_SIZE,\n",
    "            num_workers=0,\n",
    "            persistent_workers=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = ratings[\"userId\"].max() + 1\n",
    "num_items = ratings[\"movieId\"].max() + 1\n",
    "\n",
    "all_movieIds = ratings[\"movieId\"].unique()\n",
    "\n",
    "model = NCF(num_users, num_items, train_ratings, all_movieIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=r\"sandbox/weights/\", filename=\"{epoch}-{val_loss:.2f}\", monitor=\"val_loss\"\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    fast_dev_run=True,\n",
    "    max_epochs=5,\n",
    "    reload_dataloaders_every_n_epochs=1,\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    logger=False,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---DEBUG ZONE---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(r\"sandbox/weights/NCF_FINAL_epoch.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = NCF.load_from_checkpoint(\n",
    "    r\"../src/weights/epoch=2-train_loss=0.12.ckpt\",\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    ratings=train_ratings,\n",
    "    all_movieIds=all_movieIds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- END OF DEBUG ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (\n",
    "    test_ratings.groupby(\"userId\")[[\"movieId\", \"rating\"]]\n",
    "    .apply(lambda x: x.values.tolist())\n",
    "    .to_dict()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "ndcg_scores = []\n",
    "\n",
    "\n",
    "for user_id, items_targets in tqdm(grouped.items()):\n",
    "    items, true_targets = zip(*items_targets)\n",
    "\n",
    "    # Прогнозы модели для данного пользователя\n",
    "\n",
    "    items_tensor = torch.tensor(items, dtype=torch.long)\n",
    "\n",
    "    user_tensor = torch.tensor([user_id] * len(items), dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(user_tensor, items_tensor).flatten().numpy()\n",
    "\n",
    "    # Рассчитываем NDCG@20 для пользователя\n",
    "\n",
    "    if len(true_targets) > 1:  # NDCG не имеет смысла для одного элемента\n",
    "        ndcg_val = ndcg_score([true_targets], [predictions], k=20)\n",
    "\n",
    "        ndcg_scores.append(ndcg_val)\n",
    "\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores) if ndcg_scores else 0\n",
    "\n",
    "print(\"Средний NDCG@20:\", average_ndcg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
